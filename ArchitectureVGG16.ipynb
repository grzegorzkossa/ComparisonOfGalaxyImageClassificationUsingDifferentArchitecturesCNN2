{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a65deb-102e-4ad2-92f2-3ff9d340a96b",
   "metadata": {},
   "source": [
    "# Splątane sieci neuronowe CNN – architektura VGG-16 \n",
    "---\n",
    "autor: mgr inż. Grzegorz Kossakowski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de60196-61c7-4979-8274-35c7548943c0",
   "metadata": {},
   "source": [
    "## 1. Opis architektury\n",
    "Architektura została zaprojektowana przez Visual Geomerty Group (VGG) na Uniwersytecie oksfordzkim. Charakteryzuje się prostotą i skutecznością, jednocześnie umożliwia naukę skomplikowanych hierarchicznych reprezentacji cech wizualnych. Dzięki temu mimo prostoty architektury można uzyskać solidne i dokładne przewidywania.\n",
    "\n",
    "Zbudowana jest z szesnastu warstw. w tym 13 warstw splątanych i trzech warstw w pełni połączonych.\n",
    "\n",
    "VGG-16 został zaprezentowany w 2014 roku przez Karen Simonyan i Andrew Zissermana na corocznym konkursie ImageNet Large Scale Visual Recognition Challenge (ILSVRC), gdzie odniósł bardzo wielki sukces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75435b-d66d-44e6-a2fa-5ecc48e2b38c",
   "metadata": {},
   "source": [
    "## 2. Pobranie potrzebnych bibliotek\n",
    "Kolejnym krokiem jest wczytanie wszystkich potrzebnych bibliotek, dzięki którym będzie możliwe wykorzystanie ich w procesie klasyfikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82fd330-0d74-4c15-8f26-3da61c332ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "from astropy.io import fits\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras import Sequential\n",
    "from tests.test_layers import Dense, Flatten\n",
    "from keras.applications import VGG16\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c6f6c-3a97-4df3-a30b-0602b8660712",
   "metadata": {},
   "source": [
    "## 3. Ustawienie sposobu nauki\n",
    "Modele, które używany są już wstępnie wyuczone, dlatego chciałem sprawdzić, jak dany model będzie się zachowywał w dwóch przypadkach. Pierwszy przypadek gdy wartość fullStudy zostanie ustawiona na false wtedy model będzie wykorzystywał wcześniej nauczony model i na ostatnich warstwach będzie douczał tylko danymi astronomicznymi. Gdy ustawimy wartość na true, model od początku będzie, wykonał naukę architektury. Wcześniejsza nauka nie będzie brana pod uwagę. Pozwoli to ocenić, który sposób jest bardziej efektywny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7018087d-aed6-4ead-b364-224cc9290954",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullStudy = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00adab72-d8c2-4a5c-92c7-7f2d54de5cb9",
   "metadata": {},
   "source": [
    "## 4. Pobranie danych z pliku fits\n",
    "Dlatego że wcześniej podzieliliśmy dane na odpowiednie części, teraz pobieramy dwa zbiory. Pierwszy z nich to zbiór, na którym będziemy uczyć nasz model. Drugi to zbiór walidacyjny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843b6bd8-8577-4d40-8d3d-bdee4e18c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_train = fits.open('Data/train.fits')\n",
    "hdu_valid = fits.open('Data/valid.fits')\n",
    "x_train = hdu_train[0].data\n",
    "y_train = hdu_train[1].data\n",
    "x_valid = hdu_valid[0].data\n",
    "y_valid = hdu_valid[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9690d858-350e-4064-84f7-911ebab4f971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11350, 256, 256, 3), (2838, 256, 256, 3), numpy.ndarray)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape, type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6a960-35f6-46d1-bb0a-c5a67fa0d668",
   "metadata": {},
   "source": [
    "## 5. Pobranie danych \n",
    "W tym kroku pobieramy dane, a następnie przygotowujemy je do klasyfikacji. Modele głębokiej sieci neuronowej [4] wymaga danych z zakresu 0..1, dlatego wszystkie wartości w danych są dzielone przez 255. Powodem takiego zachowania jest fakt, że dane obrazów są przechowywane w zakresie liczb 0..255. Dzielenie przez 255 powoduje, że dane zostaną zapisane w zakresie od 0..1, zgodnie z wymaganiami modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13190918-6978-4357-9e7c-b92b7c2fd775",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceLR = ReduceLROnPlateau(monitor='accuracy', factor=.001, patience=1, min_delta=0.01, mode=\"auto\")\n",
    "x_train = x_train / 255.0\n",
    "x_valid = x_valid / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd9eb4f-630c-47a8-9581-ae56dfce14dc",
   "metadata": {},
   "source": [
    "## 6. Budowa modelu.\n",
    "Model w tym przypadku jest gotowy, dlatego nie musimy go budować od początku. Po dodaniu Jest to model warstwowy i jako pierwsza warstwa jest to warstwa flatten. Zadaniem tej warstwy jest spłaszczenie obrazu z wymiarów 256*256 na pojedynczy ciąg, jest to warstwa wejściowa. Kolejną warstwą jest warstwa ukryta z aktywatorem RELU. Aktywator ten powoduje, że każdy otrzymany wynik ujemy, zostaje zamieniony na zero [5][6]. Pozwala to na przełamanie liniowości procesu. Ostatnią warstwą jest gęsto połączona warstwa wyjściowa. W naszym modelu klasyfikacja odbywa się dla 10 kategorii dlatego właśnie taka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fbf1f63-8a28-4a35-a980-dc3b8b5960c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                327690    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15042378 (57.38 MB)\n",
      "Trainable params: 327690 (1.25 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256,3))\n",
    "base_model.trainable = fullStudy\n",
    "model_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=model_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559b8dc-5bd2-4c64-8269-dd11088753ca",
   "metadata": {},
   "source": [
    "## 7. Uczenie\n",
    "W tym momencie model zaczyna proces uczenia. Czyli otrzymuje dwa zbiory danych i etykiet. Pierwszy z nich to dane, na podstawie których model się uczy. Drugi mniejszy zbiór jest zbiorem walidacyjnym, który pozwala na sprawdzenie postępów w nauce, na danych, których model jeszcze nie widział. Pozwala to ocenić postępy w nauce już w czasie uczenia. Kolejny zbiór danych zostanie wykorzystany na końcu celem ostatecznego sprawdzenia poprawności działania modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd86063-8618-4297-bfbd-04b43cdbab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "355/355 [==============================] - 1062s 3s/step - loss: 1.7897 - accuracy: 0.3915 - val_loss: 1.3430 - val_accuracy: 0.5134 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "355/355 [==============================] - 1046s 3s/step - loss: 1.3396 - accuracy: 0.5339 - val_loss: 1.4697 - val_accuracy: 0.5106 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "355/355 [==============================] - 1045s 3s/step - loss: 1.1719 - accuracy: 0.5924 - val_loss: 1.2396 - val_accuracy: 0.5715 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "355/355 [==============================] - 1046s 3s/step - loss: 1.0244 - accuracy: 0.6385 - val_loss: 1.3102 - val_accuracy: 0.5444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "355/355 [==============================] - 1047s 3s/step - loss: 0.9227 - accuracy: 0.6795 - val_loss: 1.1758 - val_accuracy: 0.6025 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "355/355 [==============================] - 1046s 3s/step - loss: 0.8468 - accuracy: 0.6980 - val_loss: 1.3358 - val_accuracy: 0.5437 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "355/355 [==============================] - 1046s 3s/step - loss: 0.7490 - accuracy: 0.7407 - val_loss: 1.3680 - val_accuracy: 0.5599 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "355/355 [==============================] - 1046s 3s/step - loss: 0.7180 - accuracy: 0.7521 - val_loss: 1.3165 - val_accuracy: 0.5641 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "355/355 [==============================] - 1048s 3s/step - loss: 0.6685 - accuracy: 0.7694 - val_loss: 1.3951 - val_accuracy: 0.5511 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "355/355 [==============================] - 1049s 3s/step - loss: 0.6609 - accuracy: 0.7774 - val_loss: 1.2011 - val_accuracy: 0.6008 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, callbacks=[reduceLR],validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10682377-5fd3-41bb-86cf-ecd5122023ca",
   "metadata": {},
   "source": [
    "## 8. Zapis architektury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08dcdc15-eaf9-4d43-8ba3-07f7622d96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fullStudy == True:\n",
    "    model.save('Models/VGG16_full.keras')\n",
    "else:\n",
    "    model.save('Models/VGG16.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c425c8-409e-4d8f-ab45-4b2d14e94899",
   "metadata": {},
   "source": [
    "## 9. Zapis otrzymanych danych podczas nauki\n",
    "Po zakończeniu uczenia zapisujemy dane, które otrzymaliśmy podczas uczenie do pliku CSV. Pozwoli nam to później przeanalizować dane w późniejszym czasie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94aa1164-7182-4eda-a612-62f9c3fcac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyModelLearning = pd.DataFrame()\n",
    "historyModelLearning['loss'] = history.history['loss']\n",
    "historyModelLearning['accuracy'] = history.history['accuracy']\n",
    "historyModelLearning['val_loss'] = history.history['val_loss']\n",
    "historyModelLearning['val_accuracy'] = history.history['val_accuracy']\n",
    "if fullStudy == True:\n",
    "    historyModelLearning.to_csv('ResultLearning/VGG16_full.csv', index=True)\n",
    "else:\n",
    "    historyModelLearning.to_csv('ResultLearning/VGG16.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e63262-e19c-48a1-a8a9-a471fe379205",
   "metadata": {},
   "source": [
    "## Literatura\n",
    "1. https://www.geeksforgeeks.org/vgg-16-cnn-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6d647-b5be-4221-9044-81643f9b6e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
